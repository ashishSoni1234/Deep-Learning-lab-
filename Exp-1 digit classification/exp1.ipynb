{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd3b4b10",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-15T16:41:41.075077Z",
     "iopub.status.busy": "2025-01-15T16:41:41.074664Z",
     "iopub.status.idle": "2025-01-15T16:41:42.086830Z",
     "shell.execute_reply": "2025-01-15T16:41:42.085446Z"
    },
    "papermill": {
     "duration": 1.020524,
     "end_time": "2025-01-15T16:41:42.089806",
     "exception": false,
     "start_time": "2025-01-15T16:41:41.069282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16566c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T16:41:42.098395Z",
     "iopub.status.busy": "2025-01-15T16:41:42.097381Z",
     "iopub.status.idle": "2025-01-15T16:41:42.102663Z",
     "shell.execute_reply": "2025-01-15T16:41:42.101657Z"
    },
    "papermill": {
     "duration": 0.010544,
     "end_time": "2025-01-15T16:41:42.104356",
     "exception": false,
     "start_time": "2025-01-15T16:41:42.093812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "image_dim = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8bc728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T16:41:42.111186Z",
     "iopub.status.busy": "2025-01-15T16:41:42.110870Z",
     "iopub.status.idle": "2025-01-15T16:41:42.832148Z",
     "shell.execute_reply": "2025-01-15T16:41:42.830647Z"
    },
    "papermill": {
     "duration": 0.727719,
     "end_time": "2025-01-15T16:41:42.835012",
     "exception": false,
     "start_time": "2025-01-15T16:41:42.107293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "input_path = '/kaggle/input/mnist-dataset'\n",
    "train_image_path = join(input_path, 'train-images.idx3-ubyte')\n",
    "train_labels_path = join(input_path, 'train-labels.idx1-ubyte')\n",
    "test_image_path = join(input_path, 't10k-images.idx3-ubyte')\n",
    "test_labels_path = join(input_path, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "''' \n",
    "image is saved in a byte stream sequentially\n",
    "first 16 for header remaining 28 * 28 pixels of each image in sequence for 60000 images\n",
    "'''\n",
    "\n",
    "with open(train_image_path, 'rb') as file: \n",
    "    data = np.frombuffer(file.read(), dtype = np.uint8)\n",
    "\n",
    "with open(train_labels_path, 'rb') as file:\n",
    "    labels = np.frombuffer(file.read(), dtype = np.uint8)\n",
    "        \n",
    "labels = labels[8:]\n",
    "print(labels.shape)\n",
    "\n",
    "images = data[16:].reshape(-1,image_dim, image_dim) # original size is also (28, 28)\n",
    "print(images.shape)\n",
    "\n",
    "plt.imshow(images[1], cmap=\"gray\")\n",
    "plt.title(\"First Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5f46fa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T16:41:42.843299Z",
     "iopub.status.busy": "2025-01-15T16:41:42.842921Z",
     "iopub.status.idle": "2025-01-15T16:41:55.231762Z",
     "shell.execute_reply": "2025-01-15T16:41:55.230022Z"
    },
    "papermill": {
     "duration": 12.39615,
     "end_time": "2025-01-15T16:41:55.234591",
     "exception": false,
     "start_time": "2025-01-15T16:41:42.838441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784) (48000, 10)\n",
      "(12000, 784) (12000, 10)\n",
      "(10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Dataset Loader\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "def random_rotation(images, max_angle=15):\n",
    "    \"\"\"\n",
    "    Apply random rotations to the images.\n",
    "    :param images: 3D array of images (num_samples, height, width).\n",
    "    :param max_angle: Maximum angle for rotation.\n",
    "    :return: Rotated images.\n",
    "    \"\"\"\n",
    "    rotated_images = []\n",
    "    for image in images:\n",
    "        decision = np.random.rand()\n",
    "        if decision < 0.5: # Apply preprocessing with 50% probability\n",
    "            rotated_images.append(image)\n",
    "        else:\n",
    "            angle = np.random.uniform(-max_angle, max_angle)\n",
    "            rotated_image = rotate(image, angle, reshape=False, mode='nearest')\n",
    "            rotated_images.append(rotated_image)\n",
    "\n",
    "    return np.array(rotated_images)\n",
    "\n",
    "\n",
    "def horizontal_flip(images):\n",
    "    \"\"\"\n",
    "    Apply horizontal flip to the images.\n",
    "    :param images: 3D array of images (num_samples, height, width).\n",
    "    :return: Flipped images.\n",
    "    \"\"\"\n",
    "\n",
    "    flipped_images = []\n",
    "    for image in images:\n",
    "        decision = np.random.rand()\n",
    "        if decision < 0.5: # Apply preprocessing with 50% probability\n",
    "            flipped_images.append(image)\n",
    "        else:\n",
    "            flipped_images.append(np.fliplr(image))\n",
    "            \n",
    "    return np.array(flipped_images)\n",
    "\n",
    "class MNIST_Dataloader:\n",
    "    def __init__(self, train_image_path, train_labels_path, test_image_path, test_labels_path, dim, preprocessors=None):\n",
    "        \"\"\"\n",
    "        Initialize the MNIST dataloader.\n",
    "        :param train_image_path: Path to the training images.\n",
    "        :param train_labels_path: Path to the training labels.\n",
    "        :param test_image_path: Path to the test images.\n",
    "        :param test_labels_path: Path to the test labels.\n",
    "        :param dim: Dimension of the images (e.g., 28 for 28x28 images).\n",
    "        :param preprocessors: List of preprocessing functions to apply to the images.\n",
    "        \"\"\"\n",
    "        self.train_image_path = train_image_path\n",
    "        self.train_labels_path = train_labels_path\n",
    "        self.test_image_path = test_image_path\n",
    "        self.test_labels_path = test_labels_path\n",
    "        self.dim = dim\n",
    "        self.preprocessors = preprocessors if preprocessors is not None else []\n",
    "\n",
    "    def read_image_labels(self, image_path, label_path):\n",
    "        with open(image_path, 'rb') as file:\n",
    "            images = np.frombuffer(file.read(), dtype=np.uint8)\n",
    "\n",
    "        # Reshape and normalize images\n",
    "        images = images[16:].reshape(-1, self.dim, self.dim).astype(np.float32)\n",
    "        images /= 255.0  # Normalize to range [0, 1]\n",
    "\n",
    "        # Apply preprocessing\n",
    "        for preprocessor in self.preprocessors:\n",
    "            images = preprocessor(images)\n",
    "\n",
    "        with open(label_path, 'rb') as file:\n",
    "            labels = np.frombuffer(file.read(), dtype=np.uint8)\n",
    "\n",
    "        # One-hot encode labels\n",
    "        one_hot_labels = np.eye(10)[labels[8:]]\n",
    "\n",
    "        return images, one_hot_labels\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load the MNIST data.\n",
    "        :return: Tuple containing train and test datasets.\n",
    "        \"\"\"\n",
    "        train_images, train_labels = self.read_image_labels(self.train_image_path, self.train_labels_path)\n",
    "        test_images, test_labels = self.read_image_labels(self.test_image_path, self.test_labels_path)\n",
    "\n",
    "        # Flatten images back to 1D after preprocessing\n",
    "        train_images = train_images.reshape(-1, self.dim**2)\n",
    "        test_images = test_images.reshape(-1, self.dim**2)\n",
    "\n",
    "        return (train_images, train_labels), (test_images, test_labels)\n",
    "\n",
    "\n",
    "# Creating Loader object to get train, test datasets\n",
    "preprocessors = [random_rotation, horizontal_flip]\n",
    "dobj = MNIST_Dataloader(train_image_path, train_labels_path, test_image_path, test_labels_path, image_dim, preprocessors)\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = dobj.load_data()\n",
    "split_ratio = 0.2\n",
    "split_index = int(len(train_images) * split_ratio)\n",
    "train_images, train_labels = train_images[split_index:], train_labels[split_index:]\n",
    "val_images, val_labels = train_images[:split_index], train_labels[:split_index]\n",
    "\n",
    "print(train_images.shape, train_labels.shape)\n",
    "print(val_images.shape, val_labels.shape)\n",
    "print(test_images.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2bdd1ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T16:41:55.244199Z",
     "iopub.status.busy": "2025-01-15T16:41:55.243648Z",
     "iopub.status.idle": "2025-01-15T16:41:55.268331Z",
     "shell.execute_reply": "2025-01-15T16:41:55.266858Z"
    },
    "papermill": {
     "duration": 0.0326,
     "end_time": "2025-01-15T16:41:55.270919",
     "exception": false,
     "start_time": "2025-01-15T16:41:55.238319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FullyConnectedNN:\n",
    "    def __init__(self, layers, dropout_rate=0.0):\n",
    "        self.layers = layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.best_loss = float('inf')  # Initialize with a high value\n",
    "        \n",
    "        # Initialize weights and biases using He initialization\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.weights.append(np.random.randn(layers[i], layers[i + 1]) * np.sqrt(2 / layers[i]))\n",
    "            self.biases.append(np.zeros((1, layers[i + 1])))\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        return np.where(z > 0, 1, 0)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        # Check for NaN or Inf in z before applying softmax\n",
    "        if np.any(np.isnan(z)) or np.any(np.isinf(z)):\n",
    "            print(\"Warning: NaN or Inf detected in z before softmax.\")\n",
    "        z_stable = z - np.max(z, axis=1, keepdims=True)  # Stability improvement\n",
    "        exp_z = np.exp(z_stable)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def apply_dropout(self, activations, dropout_rate):\n",
    "        if dropout_rate > 0:\n",
    "            assert 0 <= dropout_rate < 1.0, \"Dropout rate must be between 0 and 1 (exclusive).\"\n",
    "            mask = np.random.rand(*activations.shape) > dropout_rate\n",
    "            activations *= mask\n",
    "            activations /= (1 - dropout_rate)  # Scale to maintain consistency\n",
    "            return activations, mask\n",
    "        return activations, None\n",
    "\n",
    "    def feedforward(self, X, training=True):\n",
    "        activations = [X]\n",
    "        self.dropout_masks = []  # Store dropout masks for backpropagation\n",
    "        for i in range(len(self.weights)):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            if i == len(self.weights) - 1:  # Output layer\n",
    "                a = self.softmax(z)\n",
    "            else:  # Hidden layers\n",
    "                a = self.relu(z)\n",
    "                if training:  # Apply dropout during training\n",
    "                    a, mask = self.apply_dropout(a, self.dropout_rate)\n",
    "                    self.dropout_masks.append(mask)\n",
    "                else:\n",
    "                    self.dropout_masks.append(None)  # No dropout during inference\n",
    "            activations.append(a)\n",
    "        return activations\n",
    "\n",
    "    def backpropagation(self, X, y, learning_rate):\n",
    "        activations = self.feedforward(X, training=True)\n",
    "        deltas = [activations[-1] - y]\n",
    "\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[-1], self.weights[i].T)\n",
    "            if self.dropout_masks[i - 1] is not None:\n",
    "                delta *= self.dropout_masks[i - 1]  # Apply dropout mask\n",
    "            delta *= self.relu_derivative(activations[i])\n",
    "            deltas.append(delta)\n",
    "\n",
    "        deltas.reverse()\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            # Update weights and biases with gradient clipping\n",
    "            grad_w = np.dot(activations[i].T, deltas[i])\n",
    "            grad_b = np.sum(deltas[i], axis=0, keepdims=True)\n",
    "            grad_w = np.clip(grad_w, -2.0, 2.0)  # Clip gradients to prevent explosion\n",
    "            grad_b = np.clip(grad_b, -2.0, 2.0)\n",
    "            self.weights[i] -= learning_rate * grad_w\n",
    "            self.biases[i] -= learning_rate * grad_b\n",
    "\n",
    "    def train(self, X, y, X_val, y_val, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            self.backpropagation(X, y, learning_rate)\n",
    "            train_loss = self.calculate_loss(X, y)\n",
    "            val_loss = self.calculate_loss(X_val, y_val)\n",
    "            accuracy = self.calculate_accuracy(X_val, y_val)\n",
    "\n",
    "            # Save the best weights based on validation loss\n",
    "            if val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "                best_weights = [w.copy() for w in self.weights]\n",
    "                best_biases = [b.copy() for b in self.biases]\n",
    "                print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "                self.save_best_weights('best_weights.npy', best_weights, best_biases)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.feedforward(X, training=False)[-1]\n",
    "\n",
    "    def calculate_loss(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        # Add epsilon to avoid log(0)\n",
    "        return -np.mean(np.sum(y * np.log(predictions + 1e-8), axis=1))\n",
    "\n",
    "    def calculate_accuracy(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "        true_labels = np.argmax(y, axis=1)\n",
    "        return np.mean(predicted_labels == true_labels)\n",
    "\n",
    "    def save_best_weights(self, filepath, best_weights, best_biases):\n",
    "        np.save(filepath, {'weights': best_weights, 'biases': best_biases})\n",
    "        print(\"Best weights saved to disk.\")\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        data = np.load(filepath, allow_pickle=True).item()\n",
    "        self.weights = data['weights']\n",
    "        self.biases = data['biases']\n",
    "        print(\"Best weights loaded from disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd10b053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T16:41:55.279372Z",
     "iopub.status.busy": "2025-01-15T16:41:55.279005Z",
     "iopub.status.idle": "2025-01-15T19:38:22.057870Z",
     "shell.execute_reply": "2025-01-15T19:38:22.056116Z"
    },
    "papermill": {
     "duration": 10586.78672,
     "end_time": "2025-01-15T19:38:22.061346",
     "exception": false,
     "start_time": "2025-01-15T16:41:55.274626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 723200\n",
      "Epoch 0, Train Loss: 2.1073, Val Loss: 2.1172, Accuracy: 0.1875\n",
      "Best weights saved to disk.\n",
      "Epoch 0, Train Loss: 2.1073, Val Loss: 2.1172, Accuracy: 0.1875\n",
      "Epoch 1, Train Loss: 1.9839, Val Loss: 1.9901, Accuracy: 0.3219\n",
      "Best weights saved to disk.\n",
      "Epoch 2, Train Loss: 1.8852, Val Loss: 1.8957, Accuracy: 0.4425\n",
      "Best weights saved to disk.\n",
      "Epoch 3, Train Loss: 1.8293, Val Loss: 1.8632, Accuracy: 0.3119\n",
      "Best weights saved to disk.\n",
      "Epoch 4, Train Loss: 1.7669, Val Loss: 1.7755, Accuracy: 0.4129\n",
      "Best weights saved to disk.\n",
      "Epoch 6, Train Loss: 1.6222, Val Loss: 1.6429, Accuracy: 0.5022\n",
      "Best weights saved to disk.\n",
      "Epoch 10, Train Loss: 1.6235, Val Loss: 1.6427, Accuracy: 0.4739\n",
      "Best weights saved to disk.\n",
      "Epoch 15, Train Loss: 1.4766, Val Loss: 1.5256, Accuracy: 0.5699\n",
      "Best weights saved to disk.\n",
      "Epoch 18, Train Loss: 1.4854, Val Loss: 1.5067, Accuracy: 0.5817\n",
      "Best weights saved to disk.\n",
      "Epoch 20, Train Loss: 1.4195, Val Loss: 1.4275, Accuracy: 0.5730\n",
      "Best weights saved to disk.\n",
      "Epoch 22, Train Loss: 1.2783, Val Loss: 1.2789, Accuracy: 0.5998\n",
      "Best weights saved to disk.\n",
      "Epoch 24, Train Loss: 1.2065, Val Loss: 1.2092, Accuracy: 0.6229\n",
      "Best weights saved to disk.\n",
      "Epoch 26, Train Loss: 1.1564, Val Loss: 1.1596, Accuracy: 0.6397\n",
      "Best weights saved to disk.\n",
      "Epoch 28, Train Loss: 1.1201, Val Loss: 1.1234, Accuracy: 0.6503\n",
      "Best weights saved to disk.\n",
      "Epoch 30, Train Loss: 1.0667, Val Loss: 1.0716, Accuracy: 0.6697\n",
      "Best weights saved to disk.\n",
      "Epoch 32, Train Loss: 1.0229, Val Loss: 1.0281, Accuracy: 0.6807\n",
      "Best weights saved to disk.\n",
      "Epoch 34, Train Loss: 0.9850, Val Loss: 0.9905, Accuracy: 0.6882\n",
      "Best weights saved to disk.\n",
      "Epoch 36, Train Loss: 0.9769, Val Loss: 0.9850, Accuracy: 0.6848\n",
      "Best weights saved to disk.\n",
      "Epoch 38, Train Loss: 0.9030, Val Loss: 0.9141, Accuracy: 0.7035\n",
      "Best weights saved to disk.\n",
      "Epoch 65, Train Loss: 0.8890, Val Loss: 0.8976, Accuracy: 0.7498\n",
      "Best weights saved to disk.\n",
      "Epoch 67, Train Loss: 0.8737, Val Loss: 0.8839, Accuracy: 0.7562\n",
      "Best weights saved to disk.\n",
      "Epoch 69, Train Loss: 0.8522, Val Loss: 0.8622, Accuracy: 0.7608\n",
      "Best weights saved to disk.\n",
      "Epoch 71, Train Loss: 0.8324, Val Loss: 0.8432, Accuracy: 0.7659\n",
      "Best weights saved to disk.\n",
      "Epoch 73, Train Loss: 0.8231, Val Loss: 0.8344, Accuracy: 0.7678\n",
      "Best weights saved to disk.\n",
      "Epoch 75, Train Loss: 0.8060, Val Loss: 0.8177, Accuracy: 0.7727\n",
      "Best weights saved to disk.\n",
      "Epoch 77, Train Loss: 0.7895, Val Loss: 0.8008, Accuracy: 0.7773\n",
      "Best weights saved to disk.\n",
      "Epoch 79, Train Loss: 0.7752, Val Loss: 0.7826, Accuracy: 0.7806\n",
      "Best weights saved to disk.\n",
      "Epoch 81, Train Loss: 0.7589, Val Loss: 0.7580, Accuracy: 0.7824\n",
      "Best weights saved to disk.\n",
      "Epoch 83, Train Loss: 0.7297, Val Loss: 0.7289, Accuracy: 0.7881\n",
      "Best weights saved to disk.\n",
      "Epoch 87, Train Loss: 0.7245, Val Loss: 0.7205, Accuracy: 0.7985\n",
      "Best weights saved to disk.\n",
      "Epoch 89, Train Loss: 0.7050, Val Loss: 0.7033, Accuracy: 0.8037\n",
      "Best weights saved to disk.\n",
      "Epoch 91, Train Loss: 0.6786, Val Loss: 0.6768, Accuracy: 0.8091\n",
      "Best weights saved to disk.\n",
      "Epoch 93, Train Loss: 0.6585, Val Loss: 0.6540, Accuracy: 0.8098\n",
      "Best weights saved to disk.\n",
      "Epoch 100, Train Loss: 1.0525, Val Loss: 1.0558, Accuracy: 0.6926\n",
      "Epoch 111, Train Loss: 0.6578, Val Loss: 0.6524, Accuracy: 0.7980\n",
      "Best weights saved to disk.\n",
      "Epoch 113, Train Loss: 0.6049, Val Loss: 0.6027, Accuracy: 0.8072\n",
      "Best weights saved to disk.\n",
      "Epoch 115, Train Loss: 0.5810, Val Loss: 0.5772, Accuracy: 0.8176\n",
      "Best weights saved to disk.\n",
      "Epoch 117, Train Loss: 0.5687, Val Loss: 0.5635, Accuracy: 0.8249\n",
      "Best weights saved to disk.\n",
      "Epoch 143, Train Loss: 0.5493, Val Loss: 0.5509, Accuracy: 0.8446\n",
      "Best weights saved to disk.\n",
      "Epoch 145, Train Loss: 0.5213, Val Loss: 0.5276, Accuracy: 0.8501\n",
      "Best weights saved to disk.\n",
      "Epoch 200, Train Loss: 0.6050, Val Loss: 0.6021, Accuracy: 0.8257\n",
      "Epoch 203, Train Loss: 0.5216, Val Loss: 0.5246, Accuracy: 0.8532\n",
      "Best weights saved to disk.\n",
      "Epoch 205, Train Loss: 0.5142, Val Loss: 0.5150, Accuracy: 0.8534\n",
      "Best weights saved to disk.\n",
      "Epoch 207, Train Loss: 0.5113, Val Loss: 0.5091, Accuracy: 0.8560\n",
      "Best weights saved to disk.\n",
      "Epoch 245, Train Loss: 0.5073, Val Loss: 0.5066, Accuracy: 0.8638\n",
      "Best weights saved to disk.\n",
      "Epoch 265, Train Loss: 0.5106, Val Loss: 0.5022, Accuracy: 0.8598\n",
      "Best weights saved to disk.\n",
      "Epoch 267, Train Loss: 0.5030, Val Loss: 0.4956, Accuracy: 0.8612\n",
      "Best weights saved to disk.\n",
      "Epoch 271, Train Loss: 0.4787, Val Loss: 0.4732, Accuracy: 0.8672\n",
      "Best weights saved to disk.\n",
      "Epoch 273, Train Loss: 0.4663, Val Loss: 0.4603, Accuracy: 0.8707\n",
      "Best weights saved to disk.\n",
      "Epoch 275, Train Loss: 0.4565, Val Loss: 0.4507, Accuracy: 0.8734\n",
      "Best weights saved to disk.\n",
      "Epoch 297, Train Loss: 0.4559, Val Loss: 0.4494, Accuracy: 0.8626\n",
      "Best weights saved to disk.\n",
      "Epoch 299, Train Loss: 0.4480, Val Loss: 0.4417, Accuracy: 0.8639\n",
      "Best weights saved to disk.\n",
      "Epoch 300, Train Loss: 0.4805, Val Loss: 0.4757, Accuracy: 0.8491\n",
      "Epoch 301, Train Loss: 0.4417, Val Loss: 0.4344, Accuracy: 0.8670\n",
      "Best weights saved to disk.\n",
      "Epoch 303, Train Loss: 0.4382, Val Loss: 0.4311, Accuracy: 0.8678\n",
      "Best weights saved to disk.\n",
      "Epoch 305, Train Loss: 0.4302, Val Loss: 0.4292, Accuracy: 0.8676\n",
      "Best weights saved to disk.\n",
      "Epoch 318, Train Loss: 0.4194, Val Loss: 0.4252, Accuracy: 0.8752\n",
      "Best weights saved to disk.\n",
      "Epoch 320, Train Loss: 0.4137, Val Loss: 0.4192, Accuracy: 0.8799\n",
      "Best weights saved to disk.\n",
      "Epoch 322, Train Loss: 0.3996, Val Loss: 0.4076, Accuracy: 0.8819\n",
      "Best weights saved to disk.\n",
      "Epoch 324, Train Loss: 0.3927, Val Loss: 0.3985, Accuracy: 0.8848\n",
      "Best weights saved to disk.\n",
      "Epoch 378, Train Loss: 0.4034, Val Loss: 0.3978, Accuracy: 0.8751\n",
      "Best weights saved to disk.\n",
      "Epoch 379, Train Loss: 0.4012, Val Loss: 0.3971, Accuracy: 0.8819\n",
      "Best weights saved to disk.\n",
      "Epoch 380, Train Loss: 0.3991, Val Loss: 0.3938, Accuracy: 0.8753\n",
      "Best weights saved to disk.\n",
      "Epoch 382, Train Loss: 0.3941, Val Loss: 0.3884, Accuracy: 0.8778\n",
      "Best weights saved to disk.\n",
      "Epoch 384, Train Loss: 0.3936, Val Loss: 0.3879, Accuracy: 0.8802\n",
      "Best weights saved to disk.\n",
      "Epoch 386, Train Loss: 0.3908, Val Loss: 0.3873, Accuracy: 0.8793\n",
      "Best weights saved to disk.\n",
      "Epoch 394, Train Loss: 0.3794, Val Loss: 0.3853, Accuracy: 0.8779\n",
      "Best weights saved to disk.\n",
      "Epoch 396, Train Loss: 0.3747, Val Loss: 0.3785, Accuracy: 0.8805\n",
      "Best weights saved to disk.\n",
      "Epoch 400, Train Loss: 0.4095, Val Loss: 0.4151, Accuracy: 0.8782\n",
      "Epoch 414, Train Loss: 0.3785, Val Loss: 0.3773, Accuracy: 0.8912\n",
      "Best weights saved to disk.\n",
      "Epoch 416, Train Loss: 0.3753, Val Loss: 0.3746, Accuracy: 0.8912\n",
      "Best weights saved to disk.\n",
      "Epoch 418, Train Loss: 0.3722, Val Loss: 0.3704, Accuracy: 0.8929\n",
      "Best weights saved to disk.\n",
      "Epoch 420, Train Loss: 0.3706, Val Loss: 0.3686, Accuracy: 0.8938\n",
      "Best weights saved to disk.\n",
      "Epoch 422, Train Loss: 0.3689, Val Loss: 0.3670, Accuracy: 0.8941\n",
      "Best weights saved to disk.\n",
      "Epoch 424, Train Loss: 0.3667, Val Loss: 0.3640, Accuracy: 0.8945\n",
      "Best weights saved to disk.\n",
      "Epoch 426, Train Loss: 0.3654, Val Loss: 0.3631, Accuracy: 0.8948\n",
      "Best weights saved to disk.\n",
      "Epoch 428, Train Loss: 0.3637, Val Loss: 0.3610, Accuracy: 0.8962\n",
      "Best weights saved to disk.\n",
      "Epoch 430, Train Loss: 0.3612, Val Loss: 0.3577, Accuracy: 0.8963\n",
      "Best weights saved to disk.\n",
      "Epoch 432, Train Loss: 0.3606, Val Loss: 0.3567, Accuracy: 0.8970\n",
      "Best weights saved to disk.\n",
      "Epoch 455, Train Loss: 0.3439, Val Loss: 0.3552, Accuracy: 0.9002\n",
      "Best weights saved to disk.\n",
      "Epoch 457, Train Loss: 0.3408, Val Loss: 0.3516, Accuracy: 0.9008\n",
      "Best weights saved to disk.\n",
      "Epoch 459, Train Loss: 0.3395, Val Loss: 0.3498, Accuracy: 0.9003\n",
      "Best weights saved to disk.\n",
      "Epoch 464, Train Loss: 0.3561, Val Loss: 0.3496, Accuracy: 0.8979\n",
      "Best weights saved to disk.\n",
      "Epoch 465, Train Loss: 0.3405, Val Loss: 0.3495, Accuracy: 0.8992\n",
      "Best weights saved to disk.\n",
      "Epoch 466, Train Loss: 0.3531, Val Loss: 0.3462, Accuracy: 0.8968\n",
      "Best weights saved to disk.\n",
      "Epoch 481, Train Loss: 0.3429, Val Loss: 0.3403, Accuracy: 0.8937\n",
      "Best weights saved to disk.\n",
      "Epoch 483, Train Loss: 0.3374, Val Loss: 0.3356, Accuracy: 0.8940\n",
      "Best weights saved to disk.\n",
      "Epoch 485, Train Loss: 0.3315, Val Loss: 0.3283, Accuracy: 0.8959\n",
      "Best weights saved to disk.\n",
      "Epoch 487, Train Loss: 0.3265, Val Loss: 0.3239, Accuracy: 0.8975\n",
      "Best weights saved to disk.\n",
      "Epoch 491, Train Loss: 0.3200, Val Loss: 0.3204, Accuracy: 0.8989\n",
      "Best weights saved to disk.\n",
      "Epoch 493, Train Loss: 0.3137, Val Loss: 0.3150, Accuracy: 0.8995\n",
      "Best weights saved to disk.\n",
      "Epoch 495, Train Loss: 0.3082, Val Loss: 0.3080, Accuracy: 0.9014\n",
      "Best weights saved to disk.\n",
      "Epoch 497, Train Loss: 0.3006, Val Loss: 0.3010, Accuracy: 0.9038\n",
      "Best weights saved to disk.\n",
      "Epoch 500, Train Loss: 0.3349, Val Loss: 0.3303, Accuracy: 0.8943\n",
      "Epoch 509, Train Loss: 0.3022, Val Loss: 0.2961, Accuracy: 0.9083\n",
      "Best weights saved to disk.\n",
      "Epoch 511, Train Loss: 0.2973, Val Loss: 0.2925, Accuracy: 0.9108\n",
      "Best weights saved to disk.\n",
      "Epoch 513, Train Loss: 0.2938, Val Loss: 0.2894, Accuracy: 0.9105\n",
      "Best weights saved to disk.\n",
      "Epoch 515, Train Loss: 0.2949, Val Loss: 0.2888, Accuracy: 0.9107\n",
      "Best weights saved to disk.\n",
      "Epoch 550, Train Loss: 0.2918, Val Loss: 0.2881, Accuracy: 0.9112\n",
      "Best weights saved to disk.\n",
      "Epoch 552, Train Loss: 0.2909, Val Loss: 0.2872, Accuracy: 0.9111\n",
      "Best weights saved to disk.\n",
      "Epoch 554, Train Loss: 0.2863, Val Loss: 0.2838, Accuracy: 0.9116\n",
      "Best weights saved to disk.\n",
      "Epoch 556, Train Loss: 0.2826, Val Loss: 0.2806, Accuracy: 0.9132\n",
      "Best weights saved to disk.\n",
      "Epoch 560, Train Loss: 0.2826, Val Loss: 0.2803, Accuracy: 0.9144\n",
      "Best weights saved to disk.\n",
      "Epoch 563, Train Loss: 0.2772, Val Loss: 0.2785, Accuracy: 0.9157\n",
      "Best weights saved to disk.\n",
      "Epoch 565, Train Loss: 0.2635, Val Loss: 0.2636, Accuracy: 0.9184\n",
      "Best weights saved to disk.\n",
      "Epoch 567, Train Loss: 0.2542, Val Loss: 0.2545, Accuracy: 0.9216\n",
      "Best weights saved to disk.\n",
      "Epoch 569, Train Loss: 0.2515, Val Loss: 0.2510, Accuracy: 0.9223\n",
      "Best weights saved to disk.\n",
      "Epoch 600, Train Loss: 0.2833, Val Loss: 0.2808, Accuracy: 0.9146\n",
      "Epoch 685, Train Loss: 0.2493, Val Loss: 0.2474, Accuracy: 0.9260\n",
      "Best weights saved to disk.\n",
      "Epoch 687, Train Loss: 0.2467, Val Loss: 0.2442, Accuracy: 0.9273\n",
      "Best weights saved to disk.\n",
      "Epoch 689, Train Loss: 0.2457, Val Loss: 0.2424, Accuracy: 0.9276\n",
      "Best weights saved to disk.\n",
      "Epoch 691, Train Loss: 0.2440, Val Loss: 0.2415, Accuracy: 0.9280\n",
      "Best weights saved to disk.\n",
      "Epoch 700, Train Loss: 0.2684, Val Loss: 0.2593, Accuracy: 0.9197\n",
      "Epoch 701, Train Loss: 0.2465, Val Loss: 0.2407, Accuracy: 0.9289\n",
      "Best weights saved to disk.\n",
      "Epoch 703, Train Loss: 0.2429, Val Loss: 0.2379, Accuracy: 0.9291\n",
      "Best weights saved to disk.\n",
      "Epoch 705, Train Loss: 0.2397, Val Loss: 0.2347, Accuracy: 0.9297\n",
      "Best weights saved to disk.\n",
      "Epoch 707, Train Loss: 0.2363, Val Loss: 0.2310, Accuracy: 0.9323\n",
      "Best weights saved to disk.\n",
      "Epoch 709, Train Loss: 0.2344, Val Loss: 0.2295, Accuracy: 0.9315\n",
      "Best weights saved to disk.\n",
      "Epoch 711, Train Loss: 0.2317, Val Loss: 0.2289, Accuracy: 0.9328\n",
      "Best weights saved to disk.\n",
      "Epoch 713, Train Loss: 0.2289, Val Loss: 0.2272, Accuracy: 0.9322\n",
      "Best weights saved to disk.\n",
      "Epoch 715, Train Loss: 0.2265, Val Loss: 0.2246, Accuracy: 0.9333\n",
      "Best weights saved to disk.\n",
      "Epoch 717, Train Loss: 0.2258, Val Loss: 0.2241, Accuracy: 0.9330\n",
      "Best weights saved to disk.\n",
      "Epoch 719, Train Loss: 0.2245, Val Loss: 0.2224, Accuracy: 0.9331\n",
      "Best weights saved to disk.\n",
      "Epoch 721, Train Loss: 0.2229, Val Loss: 0.2205, Accuracy: 0.9330\n",
      "Best weights saved to disk.\n",
      "Epoch 723, Train Loss: 0.2216, Val Loss: 0.2187, Accuracy: 0.9352\n",
      "Best weights saved to disk.\n",
      "Epoch 725, Train Loss: 0.2210, Val Loss: 0.2186, Accuracy: 0.9345\n",
      "Best weights saved to disk.\n",
      "Epoch 727, Train Loss: 0.2200, Val Loss: 0.2166, Accuracy: 0.9343\n",
      "Best weights saved to disk.\n",
      "Epoch 729, Train Loss: 0.2187, Val Loss: 0.2155, Accuracy: 0.9356\n",
      "Best weights saved to disk.\n",
      "Epoch 731, Train Loss: 0.2171, Val Loss: 0.2143, Accuracy: 0.9357\n",
      "Best weights saved to disk.\n",
      "Epoch 733, Train Loss: 0.2177, Val Loss: 0.2140, Accuracy: 0.9363\n",
      "Best weights saved to disk.\n",
      "Epoch 800, Train Loss: 0.2138, Val Loss: 0.2172, Accuracy: 0.9304\n",
      "Epoch 822, Train Loss: 0.2190, Val Loss: 0.2131, Accuracy: 0.9330\n",
      "Best weights saved to disk.\n",
      "Epoch 824, Train Loss: 0.2168, Val Loss: 0.2117, Accuracy: 0.9327\n",
      "Best weights saved to disk.\n",
      "Epoch 826, Train Loss: 0.2136, Val Loss: 0.2083, Accuracy: 0.9335\n",
      "Best weights saved to disk.\n",
      "Epoch 828, Train Loss: 0.2117, Val Loss: 0.2064, Accuracy: 0.9341\n",
      "Best weights saved to disk.\n",
      "Epoch 830, Train Loss: 0.2104, Val Loss: 0.2053, Accuracy: 0.9348\n",
      "Best weights saved to disk.\n",
      "Epoch 832, Train Loss: 0.2084, Val Loss: 0.2030, Accuracy: 0.9341\n",
      "Best weights saved to disk.\n",
      "Epoch 834, Train Loss: 0.2065, Val Loss: 0.2006, Accuracy: 0.9357\n",
      "Best weights saved to disk.\n",
      "Epoch 836, Train Loss: 0.2040, Val Loss: 0.1977, Accuracy: 0.9366\n",
      "Best weights saved to disk.\n",
      "Epoch 838, Train Loss: 0.2023, Val Loss: 0.1958, Accuracy: 0.9369\n",
      "Best weights saved to disk.\n",
      "Epoch 840, Train Loss: 0.2002, Val Loss: 0.1932, Accuracy: 0.9383\n",
      "Best weights saved to disk.\n",
      "Epoch 842, Train Loss: 0.1997, Val Loss: 0.1932, Accuracy: 0.9376\n",
      "Best weights saved to disk.\n",
      "Epoch 844, Train Loss: 0.1994, Val Loss: 0.1927, Accuracy: 0.9379\n",
      "Best weights saved to disk.\n",
      "Epoch 846, Train Loss: 0.1983, Val Loss: 0.1909, Accuracy: 0.9383\n",
      "Best weights saved to disk.\n",
      "Epoch 848, Train Loss: 0.1962, Val Loss: 0.1892, Accuracy: 0.9380\n",
      "Best weights saved to disk.\n",
      "Epoch 850, Train Loss: 0.1949, Val Loss: 0.1879, Accuracy: 0.9391\n",
      "Best weights saved to disk.\n",
      "Epoch 900, Train Loss: 0.2368, Val Loss: 0.2286, Accuracy: 0.9301\n",
      "Epoch 966, Train Loss: 0.1906, Val Loss: 0.1868, Accuracy: 0.9416\n",
      "Best weights saved to disk.\n",
      "Epoch 968, Train Loss: 0.1876, Val Loss: 0.1836, Accuracy: 0.9417\n",
      "Best weights saved to disk.\n",
      "Epoch 970, Train Loss: 0.1857, Val Loss: 0.1810, Accuracy: 0.9435\n",
      "Best weights saved to disk.\n",
      "Epoch 972, Train Loss: 0.1832, Val Loss: 0.1785, Accuracy: 0.9436\n",
      "Best weights saved to disk.\n",
      "Epoch 974, Train Loss: 0.1810, Val Loss: 0.1767, Accuracy: 0.9443\n",
      "Best weights saved to disk.\n",
      "Epoch 976, Train Loss: 0.1808, Val Loss: 0.1764, Accuracy: 0.9450\n",
      "Best weights saved to disk.\n",
      "Epoch 978, Train Loss: 0.1807, Val Loss: 0.1762, Accuracy: 0.9443\n",
      "Best weights saved to disk.\n",
      "Epoch 980, Train Loss: 0.1800, Val Loss: 0.1746, Accuracy: 0.9437\n",
      "Best weights saved to disk.\n",
      "Epoch 1000, Train Loss: 0.2074, Val Loss: 0.2064, Accuracy: 0.9325\n",
      "Epoch 1064, Train Loss: 0.1780, Val Loss: 0.1739, Accuracy: 0.9446\n",
      "Best weights saved to disk.\n",
      "Epoch 1066, Train Loss: 0.1749, Val Loss: 0.1702, Accuracy: 0.9451\n",
      "Best weights saved to disk.\n",
      "Epoch 1068, Train Loss: 0.1726, Val Loss: 0.1677, Accuracy: 0.9462\n",
      "Best weights saved to disk.\n",
      "Epoch 1070, Train Loss: 0.1721, Val Loss: 0.1676, Accuracy: 0.9471\n",
      "Best weights saved to disk.\n",
      "Epoch 1072, Train Loss: 0.1708, Val Loss: 0.1665, Accuracy: 0.9466\n",
      "Best weights saved to disk.\n",
      "Epoch 1074, Train Loss: 0.1705, Val Loss: 0.1661, Accuracy: 0.9462\n",
      "Best weights saved to disk.\n",
      "Epoch 1078, Train Loss: 0.1697, Val Loss: 0.1655, Accuracy: 0.9468\n",
      "Best weights saved to disk.\n",
      "Epoch 1080, Train Loss: 0.1678, Val Loss: 0.1646, Accuracy: 0.9467\n",
      "Best weights saved to disk.\n",
      "Epoch 1082, Train Loss: 0.1672, Val Loss: 0.1638, Accuracy: 0.9475\n",
      "Best weights saved to disk.\n",
      "Epoch 1086, Train Loss: 0.1665, Val Loss: 0.1627, Accuracy: 0.9480\n",
      "Best weights saved to disk.\n",
      "Epoch 1100, Train Loss: 0.1971, Val Loss: 0.1962, Accuracy: 0.9398\n",
      "Epoch 1170, Train Loss: 0.1625, Val Loss: 0.1626, Accuracy: 0.9483\n",
      "Best weights saved to disk.\n",
      "Epoch 1172, Train Loss: 0.1611, Val Loss: 0.1611, Accuracy: 0.9488\n",
      "Best weights saved to disk.\n",
      "Epoch 1179, Train Loss: 0.1615, Val Loss: 0.1609, Accuracy: 0.9479\n",
      "Best weights saved to disk.\n",
      "Epoch 1183, Train Loss: 0.1591, Val Loss: 0.1579, Accuracy: 0.9495\n",
      "Best weights saved to disk.\n",
      "Epoch 1187, Train Loss: 0.1643, Val Loss: 0.1574, Accuracy: 0.9511\n",
      "Best weights saved to disk.\n",
      "Epoch 1200, Train Loss: 0.1964, Val Loss: 0.1942, Accuracy: 0.9383\n",
      "Epoch 1209, Train Loss: 0.1620, Val Loss: 0.1570, Accuracy: 0.9536\n",
      "Best weights saved to disk.\n",
      "Epoch 1211, Train Loss: 0.1617, Val Loss: 0.1561, Accuracy: 0.9532\n",
      "Best weights saved to disk.\n",
      "Epoch 1215, Train Loss: 0.1608, Val Loss: 0.1558, Accuracy: 0.9532\n",
      "Best weights saved to disk.\n",
      "Epoch 1217, Train Loss: 0.1595, Val Loss: 0.1544, Accuracy: 0.9531\n",
      "Best weights saved to disk.\n",
      "Epoch 1219, Train Loss: 0.1562, Val Loss: 0.1502, Accuracy: 0.9535\n",
      "Best weights saved to disk.\n",
      "Epoch 1221, Train Loss: 0.1537, Val Loss: 0.1475, Accuracy: 0.9538\n",
      "Best weights saved to disk.\n",
      "Epoch 1223, Train Loss: 0.1527, Val Loss: 0.1472, Accuracy: 0.9533\n",
      "Best weights saved to disk.\n",
      "Epoch 1225, Train Loss: 0.1512, Val Loss: 0.1469, Accuracy: 0.9546\n",
      "Best weights saved to disk.\n",
      "Epoch 1300, Train Loss: 0.1556, Val Loss: 0.1550, Accuracy: 0.9503\n",
      "Epoch 1305, Train Loss: 0.1464, Val Loss: 0.1468, Accuracy: 0.9537\n",
      "Best weights saved to disk.\n",
      "Epoch 1307, Train Loss: 0.1453, Val Loss: 0.1464, Accuracy: 0.9537\n",
      "Best weights saved to disk.\n",
      "Epoch 1309, Train Loss: 0.1448, Val Loss: 0.1461, Accuracy: 0.9532\n",
      "Best weights saved to disk.\n",
      "Epoch 1311, Train Loss: 0.1449, Val Loss: 0.1459, Accuracy: 0.9533\n",
      "Best weights saved to disk.\n",
      "Epoch 1313, Train Loss: 0.1442, Val Loss: 0.1447, Accuracy: 0.9533\n",
      "Best weights saved to disk.\n",
      "Epoch 1317, Train Loss: 0.1429, Val Loss: 0.1440, Accuracy: 0.9537\n",
      "Best weights saved to disk.\n",
      "Epoch 1319, Train Loss: 0.1428, Val Loss: 0.1434, Accuracy: 0.9545\n",
      "Best weights saved to disk.\n",
      "Epoch 1321, Train Loss: 0.1406, Val Loss: 0.1407, Accuracy: 0.9555\n",
      "Best weights saved to disk.\n",
      "Epoch 1325, Train Loss: 0.1388, Val Loss: 0.1399, Accuracy: 0.9561\n",
      "Best weights saved to disk.\n",
      "Epoch 1327, Train Loss: 0.1378, Val Loss: 0.1378, Accuracy: 0.9566\n",
      "Best weights saved to disk.\n",
      "Epoch 1345, Train Loss: 0.1393, Val Loss: 0.1371, Accuracy: 0.9537\n",
      "Best weights saved to disk.\n",
      "Epoch 1347, Train Loss: 0.1382, Val Loss: 0.1354, Accuracy: 0.9547\n",
      "Best weights saved to disk.\n",
      "Epoch 1349, Train Loss: 0.1358, Val Loss: 0.1332, Accuracy: 0.9563\n",
      "Best weights saved to disk.\n",
      "Epoch 1351, Train Loss: 0.1340, Val Loss: 0.1319, Accuracy: 0.9564\n",
      "Best weights saved to disk.\n",
      "Epoch 1353, Train Loss: 0.1308, Val Loss: 0.1297, Accuracy: 0.9582\n",
      "Best weights saved to disk.\n",
      "Epoch 1355, Train Loss: 0.1294, Val Loss: 0.1293, Accuracy: 0.9572\n",
      "Best weights saved to disk.\n",
      "Epoch 1357, Train Loss: 0.1288, Val Loss: 0.1292, Accuracy: 0.9567\n",
      "Best weights saved to disk.\n",
      "Epoch 1359, Train Loss: 0.1287, Val Loss: 0.1290, Accuracy: 0.9570\n",
      "Best weights saved to disk.\n",
      "Epoch 1361, Train Loss: 0.1274, Val Loss: 0.1281, Accuracy: 0.9573\n",
      "Best weights saved to disk.\n",
      "Epoch 1363, Train Loss: 0.1266, Val Loss: 0.1264, Accuracy: 0.9581\n",
      "Best weights saved to disk.\n",
      "Epoch 1367, Train Loss: 0.1256, Val Loss: 0.1258, Accuracy: 0.9580\n",
      "Best weights saved to disk.\n",
      "Epoch 1369, Train Loss: 0.1246, Val Loss: 0.1249, Accuracy: 0.9582\n",
      "Best weights saved to disk.\n",
      "Epoch 1371, Train Loss: 0.1244, Val Loss: 0.1245, Accuracy: 0.9580\n",
      "Best weights saved to disk.\n",
      "Epoch 1373, Train Loss: 0.1231, Val Loss: 0.1230, Accuracy: 0.9588\n",
      "Best weights saved to disk.\n",
      "Epoch 1375, Train Loss: 0.1233, Val Loss: 0.1228, Accuracy: 0.9588\n",
      "Best weights saved to disk.\n",
      "Epoch 1381, Train Loss: 0.1224, Val Loss: 0.1224, Accuracy: 0.9595\n",
      "Best weights saved to disk.\n",
      "Epoch 1383, Train Loss: 0.1210, Val Loss: 0.1216, Accuracy: 0.9601\n",
      "Best weights saved to disk.\n",
      "Epoch 1385, Train Loss: 0.1205, Val Loss: 0.1205, Accuracy: 0.9606\n",
      "Best weights saved to disk.\n",
      "Epoch 1387, Train Loss: 0.1201, Val Loss: 0.1201, Accuracy: 0.9598\n",
      "Best weights saved to disk.\n",
      "Epoch 1389, Train Loss: 0.1191, Val Loss: 0.1190, Accuracy: 0.9610\n",
      "Best weights saved to disk.\n",
      "Epoch 1391, Train Loss: 0.1192, Val Loss: 0.1188, Accuracy: 0.9603\n",
      "Best weights saved to disk.\n",
      "Epoch 1393, Train Loss: 0.1187, Val Loss: 0.1183, Accuracy: 0.9603\n",
      "Best weights saved to disk.\n",
      "Epoch 1395, Train Loss: 0.1169, Val Loss: 0.1167, Accuracy: 0.9615\n",
      "Best weights saved to disk.\n",
      "Epoch 1400, Train Loss: 0.1549, Val Loss: 0.1508, Accuracy: 0.9527\n",
      "Epoch 1401, Train Loss: 0.1161, Val Loss: 0.1165, Accuracy: 0.9626\n",
      "Best weights saved to disk.\n",
      "Epoch 1403, Train Loss: 0.1159, Val Loss: 0.1164, Accuracy: 0.9632\n",
      "Best weights saved to disk.\n",
      "Epoch 1405, Train Loss: 0.1151, Val Loss: 0.1153, Accuracy: 0.9625\n",
      "Best weights saved to disk.\n",
      "Epoch 1407, Train Loss: 0.1135, Val Loss: 0.1134, Accuracy: 0.9638\n",
      "Best weights saved to disk.\n",
      "Epoch 1409, Train Loss: 0.1126, Val Loss: 0.1118, Accuracy: 0.9641\n",
      "Best weights saved to disk.\n",
      "Epoch 1417, Train Loss: 0.1118, Val Loss: 0.1113, Accuracy: 0.9641\n",
      "Best weights saved to disk.\n",
      "Epoch 1419, Train Loss: 0.1104, Val Loss: 0.1095, Accuracy: 0.9643\n",
      "Best weights saved to disk.\n",
      "Epoch 1423, Train Loss: 0.1099, Val Loss: 0.1092, Accuracy: 0.9647\n",
      "Best weights saved to disk.\n",
      "Epoch 1443, Train Loss: 0.1093, Val Loss: 0.1088, Accuracy: 0.9635\n",
      "Best weights saved to disk.\n",
      "Epoch 1447, Train Loss: 0.1086, Val Loss: 0.1085, Accuracy: 0.9634\n",
      "Best weights saved to disk.\n",
      "Epoch 1449, Train Loss: 0.1080, Val Loss: 0.1076, Accuracy: 0.9643\n",
      "Best weights saved to disk.\n",
      "Epoch 1451, Train Loss: 0.1079, Val Loss: 0.1073, Accuracy: 0.9652\n",
      "Best weights saved to disk.\n",
      "Epoch 1500, Train Loss: 0.1434, Val Loss: 0.1405, Accuracy: 0.9533\n",
      "Epoch 1600, Train Loss: 0.1110, Val Loss: 0.1105, Accuracy: 0.9644\n",
      "Epoch 1608, Train Loss: 0.1076, Val Loss: 0.1070, Accuracy: 0.9645\n",
      "Best weights saved to disk.\n",
      "Epoch 1610, Train Loss: 0.1073, Val Loss: 0.1065, Accuracy: 0.9644\n",
      "Best weights saved to disk.\n",
      "Epoch 1612, Train Loss: 0.1059, Val Loss: 0.1049, Accuracy: 0.9652\n",
      "Best weights saved to disk.\n",
      "Epoch 1614, Train Loss: 0.1054, Val Loss: 0.1045, Accuracy: 0.9657\n",
      "Best weights saved to disk.\n",
      "Epoch 1616, Train Loss: 0.1044, Val Loss: 0.1036, Accuracy: 0.9661\n",
      "Best weights saved to disk.\n",
      "Epoch 1620, Train Loss: 0.1041, Val Loss: 0.1036, Accuracy: 0.9663\n",
      "Best weights saved to disk.\n",
      "Epoch 1622, Train Loss: 0.1038, Val Loss: 0.1036, Accuracy: 0.9663\n",
      "Best weights saved to disk.\n",
      "Epoch 1624, Train Loss: 0.1035, Val Loss: 0.1032, Accuracy: 0.9663\n",
      "Best weights saved to disk.\n",
      "Epoch 1626, Train Loss: 0.1029, Val Loss: 0.1025, Accuracy: 0.9672\n",
      "Best weights saved to disk.\n",
      "Epoch 1628, Train Loss: 0.1030, Val Loss: 0.1023, Accuracy: 0.9668\n",
      "Best weights saved to disk.\n",
      "Epoch 1634, Train Loss: 0.1020, Val Loss: 0.1012, Accuracy: 0.9669\n",
      "Best weights saved to disk.\n",
      "Epoch 1638, Train Loss: 0.1011, Val Loss: 0.1006, Accuracy: 0.9670\n",
      "Best weights saved to disk.\n",
      "Epoch 1640, Train Loss: 0.1006, Val Loss: 0.0999, Accuracy: 0.9677\n",
      "Best weights saved to disk.\n",
      "Epoch 1642, Train Loss: 0.1001, Val Loss: 0.0994, Accuracy: 0.9673\n",
      "Best weights saved to disk.\n",
      "Epoch 1644, Train Loss: 0.0996, Val Loss: 0.0993, Accuracy: 0.9679\n",
      "Best weights saved to disk.\n",
      "Epoch 1664, Train Loss: 0.0982, Val Loss: 0.0983, Accuracy: 0.9691\n",
      "Best weights saved to disk.\n",
      "Epoch 1668, Train Loss: 0.0971, Val Loss: 0.0974, Accuracy: 0.9696\n",
      "Best weights saved to disk.\n",
      "Epoch 1670, Train Loss: 0.0971, Val Loss: 0.0971, Accuracy: 0.9696\n",
      "Best weights saved to disk.\n",
      "Epoch 1674, Train Loss: 0.0955, Val Loss: 0.0954, Accuracy: 0.9692\n",
      "Best weights saved to disk.\n",
      "Epoch 1676, Train Loss: 0.0953, Val Loss: 0.0948, Accuracy: 0.9695\n",
      "Best weights saved to disk.\n",
      "Epoch 1678, Train Loss: 0.0951, Val Loss: 0.0945, Accuracy: 0.9697\n",
      "Best weights saved to disk.\n",
      "Epoch 1680, Train Loss: 0.0934, Val Loss: 0.0929, Accuracy: 0.9699\n",
      "Best weights saved to disk.\n",
      "Epoch 1684, Train Loss: 0.0932, Val Loss: 0.0922, Accuracy: 0.9706\n",
      "Best weights saved to disk.\n",
      "Epoch 1700, Train Loss: 0.1273, Val Loss: 0.1264, Accuracy: 0.9623\n",
      "Epoch 1800, Train Loss: 0.1194, Val Loss: 0.1168, Accuracy: 0.9613\n",
      "Epoch 1829, Train Loss: 0.0903, Val Loss: 0.0913, Accuracy: 0.9706\n",
      "Best weights saved to disk.\n",
      "Epoch 1900, Train Loss: 0.1094, Val Loss: 0.1067, Accuracy: 0.9642\n",
      "Epoch 1955, Train Loss: 0.0921, Val Loss: 0.0910, Accuracy: 0.9710\n",
      "Best weights saved to disk.\n",
      "Epoch 1957, Train Loss: 0.0921, Val Loss: 0.0907, Accuracy: 0.9713\n",
      "Best weights saved to disk.\n",
      "Epoch 1959, Train Loss: 0.0901, Val Loss: 0.0881, Accuracy: 0.9721\n",
      "Best weights saved to disk.\n",
      "Epoch 1979, Train Loss: 0.0856, Val Loss: 0.0868, Accuracy: 0.9719\n",
      "Best weights saved to disk.\n",
      "Epoch 1981, Train Loss: 0.0850, Val Loss: 0.0851, Accuracy: 0.9726\n",
      "Best weights saved to disk.\n",
      "Epoch 1983, Train Loss: 0.0832, Val Loss: 0.0833, Accuracy: 0.9739\n",
      "Best weights saved to disk.\n",
      "Epoch 1985, Train Loss: 0.0818, Val Loss: 0.0823, Accuracy: 0.9737\n",
      "Best weights saved to disk.\n",
      "Epoch 1993, Train Loss: 0.0816, Val Loss: 0.0820, Accuracy: 0.9737\n",
      "Best weights saved to disk.\n",
      "Epoch 1995, Train Loss: 0.0817, Val Loss: 0.0816, Accuracy: 0.9738\n",
      "Best weights saved to disk.\n",
      "Epoch 1997, Train Loss: 0.0809, Val Loss: 0.0800, Accuracy: 0.9742\n",
      "Best weights saved to disk.\n",
      "Epoch 2000, Train Loss: 0.1143, Val Loss: 0.1120, Accuracy: 0.9650\n",
      "Epoch 2100, Train Loss: 0.0955, Val Loss: 0.0948, Accuracy: 0.9686\n",
      "Epoch 2168, Train Loss: 0.0789, Val Loss: 0.0779, Accuracy: 0.9745\n",
      "Best weights saved to disk.\n",
      "Epoch 2170, Train Loss: 0.0775, Val Loss: 0.0760, Accuracy: 0.9759\n",
      "Best weights saved to disk.\n",
      "Epoch 2172, Train Loss: 0.0764, Val Loss: 0.0751, Accuracy: 0.9758\n",
      "Best weights saved to disk.\n",
      "Epoch 2174, Train Loss: 0.0761, Val Loss: 0.0749, Accuracy: 0.9761\n",
      "Best weights saved to disk.\n",
      "Epoch 2176, Train Loss: 0.0761, Val Loss: 0.0743, Accuracy: 0.9761\n",
      "Best weights saved to disk.\n",
      "Epoch 2178, Train Loss: 0.0757, Val Loss: 0.0741, Accuracy: 0.9761\n",
      "Best weights saved to disk.\n",
      "Epoch 2198, Train Loss: 0.0734, Val Loss: 0.0739, Accuracy: 0.9754\n",
      "Best weights saved to disk.\n",
      "Epoch 2200, Train Loss: 0.0745, Val Loss: 0.0748, Accuracy: 0.9738\n",
      "Epoch 2300, Train Loss: 0.0810, Val Loss: 0.0802, Accuracy: 0.9737\n",
      "Epoch 2308, Train Loss: 0.0751, Val Loss: 0.0736, Accuracy: 0.9769\n",
      "Best weights saved to disk.\n",
      "Epoch 2310, Train Loss: 0.0735, Val Loss: 0.0722, Accuracy: 0.9766\n",
      "Best weights saved to disk.\n",
      "Epoch 2320, Train Loss: 0.0738, Val Loss: 0.0722, Accuracy: 0.9768\n",
      "Best weights saved to disk.\n",
      "Epoch 2322, Train Loss: 0.0728, Val Loss: 0.0714, Accuracy: 0.9772\n",
      "Best weights saved to disk.\n",
      "Epoch 2324, Train Loss: 0.0723, Val Loss: 0.0708, Accuracy: 0.9769\n",
      "Best weights saved to disk.\n",
      "Epoch 2380, Train Loss: 0.0698, Val Loss: 0.0705, Accuracy: 0.9777\n",
      "Best weights saved to disk.\n",
      "Epoch 2382, Train Loss: 0.0696, Val Loss: 0.0704, Accuracy: 0.9778\n",
      "Best weights saved to disk.\n",
      "Epoch 2384, Train Loss: 0.0692, Val Loss: 0.0696, Accuracy: 0.9781\n",
      "Best weights saved to disk.\n",
      "Epoch 2386, Train Loss: 0.0690, Val Loss: 0.0693, Accuracy: 0.9781\n",
      "Best weights saved to disk.\n",
      "Epoch 2390, Train Loss: 0.0685, Val Loss: 0.0691, Accuracy: 0.9779\n",
      "Best weights saved to disk.\n",
      "Epoch 2392, Train Loss: 0.0669, Val Loss: 0.0667, Accuracy: 0.9788\n",
      "Best weights saved to disk.\n",
      "Epoch 2400, Train Loss: 0.0758, Val Loss: 0.0757, Accuracy: 0.9748\n",
      "Epoch 2500, Train Loss: 0.0740, Val Loss: 0.0729, Accuracy: 0.9757\n",
      "Epoch 2544, Train Loss: 0.0660, Val Loss: 0.0666, Accuracy: 0.9771\n",
      "Best weights saved to disk.\n",
      "Epoch 2546, Train Loss: 0.0656, Val Loss: 0.0658, Accuracy: 0.9778\n",
      "Best weights saved to disk.\n",
      "Epoch 2550, Train Loss: 0.0653, Val Loss: 0.0645, Accuracy: 0.9779\n",
      "Best weights saved to disk.\n",
      "Epoch 2552, Train Loss: 0.0646, Val Loss: 0.0637, Accuracy: 0.9776\n",
      "Best weights saved to disk.\n",
      "Epoch 2558, Train Loss: 0.0639, Val Loss: 0.0633, Accuracy: 0.9782\n",
      "Best weights saved to disk.\n",
      "Epoch 2600, Train Loss: 0.0776, Val Loss: 0.0765, Accuracy: 0.9757\n",
      "Epoch 2700, Train Loss: 0.0672, Val Loss: 0.0664, Accuracy: 0.9780\n",
      "Epoch 2781, Train Loss: 0.0628, Val Loss: 0.0632, Accuracy: 0.9788\n",
      "Best weights saved to disk.\n",
      "Epoch 2785, Train Loss: 0.0626, Val Loss: 0.0630, Accuracy: 0.9796\n",
      "Best weights saved to disk.\n",
      "Epoch 2787, Train Loss: 0.0619, Val Loss: 0.0625, Accuracy: 0.9795\n",
      "Best weights saved to disk.\n",
      "Epoch 2789, Train Loss: 0.0619, Val Loss: 0.0622, Accuracy: 0.9792\n",
      "Best weights saved to disk.\n",
      "Epoch 2791, Train Loss: 0.0611, Val Loss: 0.0613, Accuracy: 0.9797\n",
      "Best weights saved to disk.\n",
      "Epoch 2800, Train Loss: 0.0611, Val Loss: 0.0628, Accuracy: 0.9802\n",
      "Epoch 2801, Train Loss: 0.0601, Val Loss: 0.0608, Accuracy: 0.9801\n",
      "Best weights saved to disk.\n",
      "Epoch 2809, Train Loss: 0.0595, Val Loss: 0.0604, Accuracy: 0.9801\n",
      "Best weights saved to disk.\n",
      "Epoch 2813, Train Loss: 0.0597, Val Loss: 0.0602, Accuracy: 0.9798\n",
      "Best weights saved to disk.\n",
      "Epoch 2815, Train Loss: 0.0597, Val Loss: 0.0600, Accuracy: 0.9802\n",
      "Best weights saved to disk.\n",
      "Epoch 2817, Train Loss: 0.0592, Val Loss: 0.0594, Accuracy: 0.9802\n",
      "Best weights saved to disk.\n",
      "Epoch 2819, Train Loss: 0.0592, Val Loss: 0.0592, Accuracy: 0.9800\n",
      "Best weights saved to disk.\n",
      "Epoch 2827, Train Loss: 0.0578, Val Loss: 0.0585, Accuracy: 0.9796\n",
      "Best weights saved to disk.\n",
      "Epoch 2829, Train Loss: 0.0572, Val Loss: 0.0577, Accuracy: 0.9801\n",
      "Best weights saved to disk.\n",
      "Epoch 2878, Train Loss: 0.0590, Val Loss: 0.0572, Accuracy: 0.9811\n",
      "Best weights saved to disk.\n",
      "Epoch 2880, Train Loss: 0.0585, Val Loss: 0.0569, Accuracy: 0.9815\n",
      "Best weights saved to disk.\n",
      "Epoch 2882, Train Loss: 0.0579, Val Loss: 0.0561, Accuracy: 0.9820\n",
      "Best weights saved to disk.\n",
      "Epoch 2884, Train Loss: 0.0575, Val Loss: 0.0558, Accuracy: 0.9819\n",
      "Best weights saved to disk.\n",
      "Epoch 2886, Train Loss: 0.0574, Val Loss: 0.0557, Accuracy: 0.9815\n",
      "Best weights saved to disk.\n",
      "Epoch 2888, Train Loss: 0.0570, Val Loss: 0.0556, Accuracy: 0.9818\n",
      "Best weights saved to disk.\n",
      "Epoch 2890, Train Loss: 0.0563, Val Loss: 0.0547, Accuracy: 0.9829\n",
      "Best weights saved to disk.\n",
      "Epoch 2900, Train Loss: 0.0565, Val Loss: 0.0554, Accuracy: 0.9819\n",
      "Epoch 2902, Train Loss: 0.0560, Val Loss: 0.0547, Accuracy: 0.9823\n",
      "Best weights saved to disk.\n",
      "Epoch 2906, Train Loss: 0.0549, Val Loss: 0.0535, Accuracy: 0.9828\n",
      "Best weights saved to disk.\n",
      "Epoch 2912, Train Loss: 0.0544, Val Loss: 0.0530, Accuracy: 0.9828\n",
      "Best weights saved to disk.\n",
      "Epoch 2920, Train Loss: 0.0528, Val Loss: 0.0524, Accuracy: 0.9827\n",
      "Best weights saved to disk.\n",
      "Epoch 2928, Train Loss: 0.0523, Val Loss: 0.0517, Accuracy: 0.9827\n",
      "Best weights saved to disk.\n",
      "Epoch 2930, Train Loss: 0.0519, Val Loss: 0.0512, Accuracy: 0.9829\n",
      "Best weights saved to disk.\n",
      "Epoch 3000, Train Loss: 0.0532, Val Loss: 0.0527, Accuracy: 0.9823\n",
      "Epoch 3014, Train Loss: 0.0517, Val Loss: 0.0509, Accuracy: 0.9829\n",
      "Best weights saved to disk.\n",
      "Epoch 3016, Train Loss: 0.0516, Val Loss: 0.0508, Accuracy: 0.9831\n",
      "Best weights saved to disk.\n",
      "Epoch 3020, Train Loss: 0.0515, Val Loss: 0.0507, Accuracy: 0.9835\n",
      "Best weights saved to disk.\n",
      "Epoch 3022, Train Loss: 0.0515, Val Loss: 0.0504, Accuracy: 0.9834\n",
      "Best weights saved to disk.\n",
      "Epoch 3030, Train Loss: 0.0512, Val Loss: 0.0502, Accuracy: 0.9831\n",
      "Best weights saved to disk.\n",
      "Epoch 3032, Train Loss: 0.0508, Val Loss: 0.0497, Accuracy: 0.9837\n",
      "Best weights saved to disk.\n",
      "Epoch 3036, Train Loss: 0.0502, Val Loss: 0.0496, Accuracy: 0.9834\n",
      "Best weights saved to disk.\n",
      "Epoch 3044, Train Loss: 0.0502, Val Loss: 0.0495, Accuracy: 0.9832\n",
      "Best weights saved to disk.\n",
      "Epoch 3051, Train Loss: 0.0512, Val Loss: 0.0494, Accuracy: 0.9837\n",
      "Best weights saved to disk.\n",
      "Epoch 3053, Train Loss: 0.0511, Val Loss: 0.0492, Accuracy: 0.9838\n",
      "Best weights saved to disk.\n",
      "Epoch 3058, Train Loss: 0.0496, Val Loss: 0.0492, Accuracy: 0.9839\n",
      "Best weights saved to disk.\n",
      "Epoch 3060, Train Loss: 0.0491, Val Loss: 0.0487, Accuracy: 0.9835\n",
      "Best weights saved to disk.\n",
      "Epoch 3062, Train Loss: 0.0488, Val Loss: 0.0482, Accuracy: 0.9838\n",
      "Best weights saved to disk.\n",
      "Epoch 3064, Train Loss: 0.0489, Val Loss: 0.0478, Accuracy: 0.9837\n",
      "Best weights saved to disk.\n",
      "Epoch 3070, Train Loss: 0.0477, Val Loss: 0.0476, Accuracy: 0.9846\n",
      "Best weights saved to disk.\n",
      "Epoch 3100, Train Loss: 0.0541, Val Loss: 0.0537, Accuracy: 0.9824\n",
      "Epoch 3176, Train Loss: 0.0469, Val Loss: 0.0474, Accuracy: 0.9844\n",
      "Best weights saved to disk.\n",
      "Epoch 3182, Train Loss: 0.0466, Val Loss: 0.0473, Accuracy: 0.9847\n",
      "Best weights saved to disk.\n",
      "Epoch 3186, Train Loss: 0.0463, Val Loss: 0.0470, Accuracy: 0.9846\n",
      "Best weights saved to disk.\n",
      "Epoch 3190, Train Loss: 0.0454, Val Loss: 0.0460, Accuracy: 0.9854\n",
      "Best weights saved to disk.\n",
      "Epoch 3200, Train Loss: 0.0465, Val Loss: 0.0482, Accuracy: 0.9838\n",
      "Epoch 3297, Train Loss: 0.0481, Val Loss: 0.0446, Accuracy: 0.9837\n",
      "Best weights saved to disk.\n",
      "Epoch 3300, Train Loss: 0.0508, Val Loss: 0.0491, Accuracy: 0.9848\n",
      "Epoch 3301, Train Loss: 0.0476, Val Loss: 0.0445, Accuracy: 0.9839\n",
      "Best weights saved to disk.\n",
      "Epoch 3303, Train Loss: 0.0474, Val Loss: 0.0443, Accuracy: 0.9839\n",
      "Best weights saved to disk.\n",
      "Epoch 3309, Train Loss: 0.0454, Val Loss: 0.0435, Accuracy: 0.9859\n",
      "Best weights saved to disk.\n",
      "Epoch 3343, Train Loss: 0.0443, Val Loss: 0.0433, Accuracy: 0.9848\n",
      "Best weights saved to disk.\n",
      "Epoch 3400, Train Loss: 0.0476, Val Loss: 0.0477, Accuracy: 0.9833\n",
      "Epoch 3407, Train Loss: 0.0430, Val Loss: 0.0428, Accuracy: 0.9860\n",
      "Best weights saved to disk.\n",
      "Epoch 3411, Train Loss: 0.0425, Val Loss: 0.0424, Accuracy: 0.9861\n",
      "Best weights saved to disk.\n",
      "Epoch 3413, Train Loss: 0.0425, Val Loss: 0.0422, Accuracy: 0.9868\n",
      "Best weights saved to disk.\n",
      "Epoch 3415, Train Loss: 0.0427, Val Loss: 0.0422, Accuracy: 0.9865\n",
      "Best weights saved to disk.\n",
      "Epoch 3417, Train Loss: 0.0429, Val Loss: 0.0420, Accuracy: 0.9868\n",
      "Best weights saved to disk.\n",
      "Epoch 3419, Train Loss: 0.0426, Val Loss: 0.0411, Accuracy: 0.9868\n",
      "Best weights saved to disk.\n",
      "Epoch 3500, Train Loss: 0.0428, Val Loss: 0.0410, Accuracy: 0.9867\n",
      "Best weights saved to disk.\n",
      "Epoch 3500, Train Loss: 0.0428, Val Loss: 0.0410, Accuracy: 0.9867\n",
      "Epoch 3502, Train Loss: 0.0423, Val Loss: 0.0410, Accuracy: 0.9862\n",
      "Best weights saved to disk.\n",
      "Epoch 3506, Train Loss: 0.0422, Val Loss: 0.0406, Accuracy: 0.9871\n",
      "Best weights saved to disk.\n",
      "Epoch 3508, Train Loss: 0.0414, Val Loss: 0.0402, Accuracy: 0.9867\n",
      "Best weights saved to disk.\n",
      "Epoch 3510, Train Loss: 0.0416, Val Loss: 0.0402, Accuracy: 0.9862\n",
      "Best weights saved to disk.\n",
      "Epoch 3512, Train Loss: 0.0418, Val Loss: 0.0401, Accuracy: 0.9870\n",
      "Best weights saved to disk.\n",
      "Epoch 3514, Train Loss: 0.0419, Val Loss: 0.0399, Accuracy: 0.9871\n",
      "Best weights saved to disk.\n",
      "Epoch 3539, Train Loss: 0.0409, Val Loss: 0.0397, Accuracy: 0.9877\n",
      "Best weights saved to disk.\n",
      "Epoch 3542, Train Loss: 0.0405, Val Loss: 0.0396, Accuracy: 0.9861\n",
      "Best weights saved to disk.\n",
      "Epoch 3543, Train Loss: 0.0407, Val Loss: 0.0393, Accuracy: 0.9874\n",
      "Best weights saved to disk.\n",
      "Epoch 3545, Train Loss: 0.0403, Val Loss: 0.0388, Accuracy: 0.9877\n",
      "Best weights saved to disk.\n",
      "Epoch 3557, Train Loss: 0.0401, Val Loss: 0.0387, Accuracy: 0.9878\n",
      "Best weights saved to disk.\n",
      "Epoch 3559, Train Loss: 0.0401, Val Loss: 0.0384, Accuracy: 0.9882\n",
      "Best weights saved to disk.\n",
      "Epoch 3567, Train Loss: 0.0395, Val Loss: 0.0383, Accuracy: 0.9882\n",
      "Best weights saved to disk.\n",
      "Epoch 3569, Train Loss: 0.0393, Val Loss: 0.0379, Accuracy: 0.9883\n",
      "Best weights saved to disk.\n",
      "Epoch 3600, Train Loss: 0.0469, Val Loss: 0.0468, Accuracy: 0.9850\n",
      "Epoch 3661, Train Loss: 0.0385, Val Loss: 0.0377, Accuracy: 0.9862\n",
      "Best weights saved to disk.\n",
      "Epoch 3663, Train Loss: 0.0377, Val Loss: 0.0375, Accuracy: 0.9862\n",
      "Best weights saved to disk.\n",
      "Epoch 3669, Train Loss: 0.0382, Val Loss: 0.0371, Accuracy: 0.9862\n",
      "Best weights saved to disk.\n",
      "Epoch 3677, Train Loss: 0.0388, Val Loss: 0.0371, Accuracy: 0.9862\n",
      "Best weights saved to disk.\n",
      "Epoch 3687, Train Loss: 0.0382, Val Loss: 0.0365, Accuracy: 0.9865\n",
      "Best weights saved to disk.\n",
      "Epoch 3691, Train Loss: 0.0379, Val Loss: 0.0363, Accuracy: 0.9875\n",
      "Best weights saved to disk.\n",
      "Epoch 3695, Train Loss: 0.0376, Val Loss: 0.0360, Accuracy: 0.9873\n",
      "Best weights saved to disk.\n",
      "Epoch 3699, Train Loss: 0.0367, Val Loss: 0.0348, Accuracy: 0.9878\n",
      "Best weights saved to disk.\n",
      "Epoch 3700, Train Loss: 0.0422, Val Loss: 0.0422, Accuracy: 0.9854\n",
      "Epoch 3709, Train Loss: 0.0362, Val Loss: 0.0347, Accuracy: 0.9874\n",
      "Best weights saved to disk.\n",
      "Epoch 3715, Train Loss: 0.0355, Val Loss: 0.0339, Accuracy: 0.9873\n",
      "Best weights saved to disk.\n",
      "Epoch 3717, Train Loss: 0.0350, Val Loss: 0.0335, Accuracy: 0.9875\n",
      "Best weights saved to disk.\n",
      "Epoch 3767, Train Loss: 0.0337, Val Loss: 0.0333, Accuracy: 0.9879\n",
      "Best weights saved to disk.\n",
      "Epoch 3769, Train Loss: 0.0332, Val Loss: 0.0330, Accuracy: 0.9878\n",
      "Best weights saved to disk.\n",
      "Epoch 3771, Train Loss: 0.0327, Val Loss: 0.0327, Accuracy: 0.9879\n",
      "Best weights saved to disk.\n",
      "Epoch 3795, Train Loss: 0.0324, Val Loss: 0.0325, Accuracy: 0.9878\n",
      "Best weights saved to disk.\n",
      "Epoch 3797, Train Loss: 0.0321, Val Loss: 0.0324, Accuracy: 0.9878\n",
      "Best weights saved to disk.\n",
      "Epoch 3799, Train Loss: 0.0321, Val Loss: 0.0322, Accuracy: 0.9878\n",
      "Best weights saved to disk.\n",
      "Epoch 3800, Train Loss: 0.0386, Val Loss: 0.0371, Accuracy: 0.9892\n",
      "Epoch 3805, Train Loss: 0.0319, Val Loss: 0.0316, Accuracy: 0.9885\n",
      "Best weights saved to disk.\n",
      "Epoch 3821, Train Loss: 0.0325, Val Loss: 0.0314, Accuracy: 0.9890\n",
      "Best weights saved to disk.\n",
      "Epoch 3900, Train Loss: 0.0354, Val Loss: 0.0352, Accuracy: 0.9887\n",
      "Epoch 4000, Train Loss: 0.0357, Val Loss: 0.0325, Accuracy: 0.9900\n",
      "Epoch 4018, Train Loss: 0.0347, Val Loss: 0.0312, Accuracy: 0.9905\n",
      "Best weights saved to disk.\n",
      "Epoch 4022, Train Loss: 0.0345, Val Loss: 0.0310, Accuracy: 0.9908\n",
      "Best weights saved to disk.\n",
      "Epoch 4024, Train Loss: 0.0338, Val Loss: 0.0304, Accuracy: 0.9904\n",
      "Best weights saved to disk.\n",
      "Epoch 4028, Train Loss: 0.0336, Val Loss: 0.0301, Accuracy: 0.9901\n",
      "Best weights saved to disk.\n",
      "Epoch 4034, Train Loss: 0.0324, Val Loss: 0.0294, Accuracy: 0.9904\n",
      "Best weights saved to disk.\n",
      "Epoch 4036, Train Loss: 0.0322, Val Loss: 0.0292, Accuracy: 0.9910\n",
      "Best weights saved to disk.\n",
      "Epoch 4040, Train Loss: 0.0314, Val Loss: 0.0287, Accuracy: 0.9915\n",
      "Best weights saved to disk.\n",
      "Epoch 4046, Train Loss: 0.0307, Val Loss: 0.0286, Accuracy: 0.9907\n",
      "Best weights saved to disk.\n",
      "Epoch 4066, Train Loss: 0.0297, Val Loss: 0.0283, Accuracy: 0.9914\n",
      "Best weights saved to disk.\n",
      "Epoch 4074, Train Loss: 0.0290, Val Loss: 0.0281, Accuracy: 0.9908\n",
      "Best weights saved to disk.\n",
      "Epoch 4076, Train Loss: 0.0288, Val Loss: 0.0277, Accuracy: 0.9911\n",
      "Best weights saved to disk.\n",
      "Epoch 4082, Train Loss: 0.0281, Val Loss: 0.0272, Accuracy: 0.9910\n",
      "Best weights saved to disk.\n",
      "Epoch 4090, Train Loss: 0.0280, Val Loss: 0.0272, Accuracy: 0.9914\n",
      "Best weights saved to disk.\n",
      "Epoch 4094, Train Loss: 0.0280, Val Loss: 0.0272, Accuracy: 0.9908\n",
      "Best weights saved to disk.\n",
      "Epoch 4098, Train Loss: 0.0275, Val Loss: 0.0261, Accuracy: 0.9914\n",
      "Best weights saved to disk.\n",
      "Epoch 4100, Train Loss: 0.0276, Val Loss: 0.0266, Accuracy: 0.9916\n",
      "Epoch 4102, Train Loss: 0.0276, Val Loss: 0.0260, Accuracy: 0.9912\n",
      "Best weights saved to disk.\n",
      "Epoch 4200, Train Loss: 0.0343, Val Loss: 0.0332, Accuracy: 0.9900\n",
      "Epoch 4300, Train Loss: 0.0320, Val Loss: 0.0311, Accuracy: 0.9895\n",
      "Epoch 4345, Train Loss: 0.0267, Val Loss: 0.0257, Accuracy: 0.9927\n",
      "Best weights saved to disk.\n",
      "Epoch 4400, Train Loss: 0.0270, Val Loss: 0.0261, Accuracy: 0.9924\n",
      "Epoch 4410, Train Loss: 0.0265, Val Loss: 0.0256, Accuracy: 0.9924\n",
      "Best weights saved to disk.\n",
      "Epoch 4438, Train Loss: 0.0248, Val Loss: 0.0256, Accuracy: 0.9918\n",
      "Best weights saved to disk.\n",
      "Epoch 4440, Train Loss: 0.0249, Val Loss: 0.0255, Accuracy: 0.9918\n",
      "Best weights saved to disk.\n",
      "Epoch 4492, Train Loss: 0.0268, Val Loss: 0.0249, Accuracy: 0.9931\n",
      "Best weights saved to disk.\n",
      "Epoch 4496, Train Loss: 0.0263, Val Loss: 0.0246, Accuracy: 0.9931\n",
      "Best weights saved to disk.\n",
      "Epoch 4498, Train Loss: 0.0258, Val Loss: 0.0243, Accuracy: 0.9928\n",
      "Best weights saved to disk.\n",
      "Epoch 4500, Train Loss: 0.0257, Val Loss: 0.0241, Accuracy: 0.9927\n",
      "Best weights saved to disk.\n",
      "Epoch 4500, Train Loss: 0.0257, Val Loss: 0.0241, Accuracy: 0.9927\n",
      "Epoch 4510, Train Loss: 0.0250, Val Loss: 0.0239, Accuracy: 0.9931\n",
      "Best weights saved to disk.\n",
      "Epoch 4512, Train Loss: 0.0249, Val Loss: 0.0237, Accuracy: 0.9928\n",
      "Best weights saved to disk.\n",
      "Epoch 4526, Train Loss: 0.0244, Val Loss: 0.0231, Accuracy: 0.9928\n",
      "Best weights saved to disk.\n",
      "Epoch 4528, Train Loss: 0.0239, Val Loss: 0.0228, Accuracy: 0.9931\n",
      "Best weights saved to disk.\n",
      "Epoch 4600, Train Loss: 0.0230, Val Loss: 0.0234, Accuracy: 0.9929\n",
      "Epoch 4608, Train Loss: 0.0222, Val Loss: 0.0224, Accuracy: 0.9932\n",
      "Best weights saved to disk.\n",
      "Epoch 4610, Train Loss: 0.0225, Val Loss: 0.0223, Accuracy: 0.9933\n",
      "Best weights saved to disk.\n",
      "Epoch 4612, Train Loss: 0.0225, Val Loss: 0.0220, Accuracy: 0.9931\n",
      "Best weights saved to disk.\n",
      "Epoch 4618, Train Loss: 0.0228, Val Loss: 0.0219, Accuracy: 0.9930\n",
      "Best weights saved to disk.\n",
      "Epoch 4620, Train Loss: 0.0228, Val Loss: 0.0216, Accuracy: 0.9929\n",
      "Best weights saved to disk.\n",
      "Epoch 4700, Train Loss: 0.0231, Val Loss: 0.0229, Accuracy: 0.9921\n",
      "Epoch 4708, Train Loss: 0.0219, Val Loss: 0.0213, Accuracy: 0.9932\n",
      "Best weights saved to disk.\n",
      "Epoch 4710, Train Loss: 0.0218, Val Loss: 0.0209, Accuracy: 0.9932\n",
      "Best weights saved to disk.\n",
      "Epoch 4718, Train Loss: 0.0216, Val Loss: 0.0207, Accuracy: 0.9932\n",
      "Best weights saved to disk.\n",
      "Epoch 4720, Train Loss: 0.0215, Val Loss: 0.0202, Accuracy: 0.9937\n",
      "Best weights saved to disk.\n",
      "Epoch 4783, Train Loss: 0.0206, Val Loss: 0.0199, Accuracy: 0.9927\n",
      "Best weights saved to disk.\n",
      "Epoch 4785, Train Loss: 0.0202, Val Loss: 0.0190, Accuracy: 0.9930\n",
      "Best weights saved to disk.\n",
      "Epoch 4789, Train Loss: 0.0199, Val Loss: 0.0190, Accuracy: 0.9936\n",
      "Best weights saved to disk.\n",
      "Epoch 4791, Train Loss: 0.0197, Val Loss: 0.0190, Accuracy: 0.9936\n",
      "Best weights saved to disk.\n",
      "Epoch 4793, Train Loss: 0.0196, Val Loss: 0.0188, Accuracy: 0.9940\n",
      "Best weights saved to disk.\n",
      "Epoch 4795, Train Loss: 0.0196, Val Loss: 0.0185, Accuracy: 0.9934\n",
      "Best weights saved to disk.\n",
      "Epoch 4797, Train Loss: 0.0196, Val Loss: 0.0181, Accuracy: 0.9937\n",
      "Best weights saved to disk.\n",
      "Epoch 4800, Train Loss: 0.0252, Val Loss: 0.0239, Accuracy: 0.9928\n",
      "Epoch 4900, Train Loss: 0.0201, Val Loss: 0.0207, Accuracy: 0.9941\n",
      "Epoch 4940, Train Loss: 0.0193, Val Loss: 0.0180, Accuracy: 0.9934\n",
      "Best weights saved to disk.\n",
      "Epoch 4942, Train Loss: 0.0194, Val Loss: 0.0180, Accuracy: 0.9935\n",
      "Best weights saved to disk.\n",
      "Epoch 4946, Train Loss: 0.0185, Val Loss: 0.0178, Accuracy: 0.9939\n",
      "Best weights saved to disk.\n"
     ]
    }
   ],
   "source": [
    "# Define network structure: 784 input neurons, 1 hidden layers with 128 neurons each, and 10 output neuron\n",
    "layers = [image_dim ** 2, 128, 64, 10]\n",
    "\n",
    "# Calculate the number of trainable parameters in the network\n",
    "params = 0\n",
    "for i in range(len(layers) - 1): \n",
    "    if i == 0:\n",
    "        params += layers[i] * image_dim ** 2\n",
    "    else:\n",
    "        params += layers[i] * layers[i - 1]\n",
    "\n",
    "print(\"Total trainable parameters:\", params)\n",
    "\n",
    "# Create the neural network\n",
    "nn = FullyConnectedNN(layers, 0.2)\n",
    "\n",
    "# Train the network (use train_images and train_labels)\n",
    "nn.train(train_images, train_labels, val_images, val_labels, epochs=5000, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c1f1907",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T19:38:22.188791Z",
     "iopub.status.busy": "2025-01-15T19:38:22.188379Z",
     "iopub.status.idle": "2025-01-15T19:38:22.447453Z",
     "shell.execute_reply": "2025-01-15T19:38:22.445343Z"
    },
    "papermill": {
     "duration": 0.305383,
     "end_time": "2025-01-15T19:38:22.451378",
     "exception": false,
     "start_time": "2025-01-15T19:38:22.145995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best weights loaded from disk.\n",
      "Test Accuracy: 94.99%\n",
      "Test Loss: 0.2502043822862153\n"
     ]
    }
   ],
   "source": [
    "nn.load_weights('best_weights.npy')\n",
    "accuracy = nn.calculate_accuracy(test_images, test_labels)\n",
    "loss = nn.calculate_loss(test_images, test_labels)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Test Loss: {loss}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 102285,
     "sourceId": 242592,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10605.311517,
   "end_time": "2025-01-15T19:38:23.276458",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-15T16:41:37.964941",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
